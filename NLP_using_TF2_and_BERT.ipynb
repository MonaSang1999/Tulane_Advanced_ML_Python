{"cells":[{"cell_type":"markdown","metadata":{"id":"2IjSWx7-O8yY"},"source":["# Natural Language Processing using BERT\n","\n","Please study AMA Lecture 12 \"Natural Language Processing Using BERT\" before practicing this code.\n","\n","Instead of JupyterLab, today we will use a cloud-based Python environment, **Google Colab** (https://colab.research.google.com/), which provides computing power far more than a typical laptop PC.\n"]},{"cell_type":"markdown","source":["## Notes on using Google Colab\n","\n","\"Colab notebooks are Jupyter notebooks that are hosted by Colab.\"\n","\n","Some keyboard short-cut differences -- mostly, Ctrl-M instead of Escape.\n","\n","Colab depends on Google Drive -- we'll see shortly."],"metadata":{"id":"UxvdLvNQq9Zf"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"aYS2t29Im7o1"},"outputs":[],"source":["import numpy as np\n","import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1HzU3Wionpo9"},"outputs":[],"source":["# Need tf version >=2.0\n","import tensorflow as tf\n","print(\"TF version: \", tf.__version__)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SOunH6Q4nsEf"},"outputs":[],"source":["from tensorflow.keras import models\n","from tensorflow.keras import layers\n","from tensorflow.keras import optimizers"]},{"cell_type":"markdown","source":["\n","In addition to `tensorflow` and `keras` packages, this code also requires two new packages:\n","+ `tensorflow_hub` -- \"a repository of trained machine learning models ready for fine-tuning and deployable anywhere\" (https://www.tensorflow.org/hub)\n","+ `bert`, which we'll install via \"pip install\" later in this code"],"metadata":{"id":"f44n7eSQqjBA"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"q9LGL4G6lqlG"},"outputs":[],"source":["# Need hub version >=0.7\n","import tensorflow_hub as hub\n","print(\"Hub version: \", hub.__version__)"]},{"cell_type":"markdown","metadata":{"id":"Cyn23fDFJpZ8"},"source":["## Case study: the IMDB dataset\n","\n","This is a widely used large dataset for text mining from a [2011 ACL meeting paper](https://ai.stanford.edu/~amaas/data/sentiment/) by Maas et al. I processed the data so it fits in a single CSV file 'IMDB_small.csv'.\n","\n","The original dataset has 50,000 balanced records, and the data file takes too long to upload. For our course, I randomly sampled 10,000 records and saved them in file 'IMDB_small.csv'. This is still a balanced sample, where the first 5000 are negative reviews and the rest are positive reviews."]},{"cell_type":"markdown","source":["### load the IMDB dataset if using JupyterLab"],"metadata":{"id":"jsAPeTuHphgT"}},{"cell_type":"code","source":["#df = pd.read_csv('IMDB_small.csv')"],"metadata":{"id":"jSlkijLlnXLO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### load the IMDB dataset if using Google Colab"],"metadata":{"id":"_gFRs2QMoAYj"}},{"cell_type":"markdown","source":["Colab is tightly integrated with Google Drive. To access our data already saved on Google Drive, we need to first mount Google Drive into our virtual machine in Colab:"],"metadata":{"id":"FdQ4KGMlsk2n"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"BfnQzj7om-cs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can next access data. Note that the address always starts with '/content/drive/MyDrive/':"],"metadata":{"id":"zQHmK-TTtT0i"}},{"cell_type":"code","source":["df = pd.read_csv('/content/drive/MyDrive/AMA/12_NLP_using_BERT/IMDB_small.csv')"],"metadata":{"id":"8V6urnVqnju2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Take a look at the IMDB dataset"],"metadata":{"id":"z-7uKZbxoIkS"}},{"cell_type":"code","source":["df.head()"],"metadata":{"id":"R-XQWzLYoFVo"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VBNefBTeHcqH"},"outputs":[],"source":["df.sentiment.value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K_NJpLIpSGTO"},"outputs":[],"source":["# one negative example:\n","import textwrap\n","print(textwrap.fill(df.review[0], 80))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GfXyJ2aFW0I9"},"outputs":[],"source":["# one positive example:\n","print(textwrap.fill(df.review[5000], 80))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ICTlwBaqsgZO"},"outputs":[],"source":["# The following codes make it easier for you to adopt\n","# this file for other text mining datasets.\n","DATA_COLUMN = 'review'\n","LABEL_COLUMN = 'sentiment'\n","label_list = [0, 1] #0-negative, 1-positive"]},{"cell_type":"markdown","metadata":{"id":"J5HURQ9CIdHs"},"source":["## Introducing BERT\n","\n","**BERT (Bidirectional Encoder Representations from Transformers)** is the state-of-the-art feature extraction model for natural language.\n","\n","Some resources on BERT:\n","- See BERT on paper: https://arxiv.org/pdf/1810.04805.pdf\n","- See BERT on GitHub: https://github.com/google-research/bert\n","- See BERT on TensorHub: https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\n","- See 'old' use of BERT for comparison: https://colab.research.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb"]},{"cell_type":"markdown","metadata":{"id":"1dU6PU4emdgQ"},"source":["Next, we will use BERT in four steps:\n","* Import and build the BERT model\n","* Tokenization\n","* Convert tokens to BERT input format\n","* Sentence/word embedding"]},{"cell_type":"markdown","metadata":{"id":"PtaEqbjwrI4s"},"source":["## Importing and building the BERT model\n","\n","This part of code might confuse you a bit for now. We will come back and explain it more."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cEbWuAdFqB2S"},"outputs":[],"source":["# !pip install sentencepiece\n","!pip install bert-for-tf2\n","import bert"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IUInJmh7qpvo"},"outputs":[],"source":["# BERT requires a MAX_SEQ_LENGTH that can be any integer<=512.\n","# Here we pick a smaller number to cut down computation cost.\n","max_seq_length = 256"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6IqEqHw8pxPo"},"outputs":[],"source":["# BERT requires the following three types of inputs (more on them later)\n","input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n","                                       name=\"input_word_ids\")\n","input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n","                                   name=\"input_mask\")\n","segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n","                                    name=\"segment_ids\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rs-kXm1P39Lo"},"outputs":[],"source":["# Now we load the already pre-trained BERT layers\n","# Ignore the warning message, which won't affect our usage of bert\n","bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n","                            trainable=True)\n","pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LETaImqhruXz"},"outputs":[],"source":["model = models.Model(inputs=[input_word_ids, input_mask, segment_ids], \n","                     outputs=[pooled_output, sequence_output])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cAxh7sZe3EcV"},"outputs":[],"source":["model.summary()"]},{"cell_type":"markdown","metadata":{"id":"FShEbcDZmvF_"},"source":["## BERT for tokenization"]},{"cell_type":"markdown","metadata":{"id":"44u2pruZSbMX"},"source":["Import tokenizer using the original vocab file:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sm3lGfQb-1J8"},"outputs":[],"source":["vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n","do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n","tokenizer = bert.bert_tokenization.FullTokenizer(vocab_file, do_lower_case)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5bQ-FXvtg_tY"},"outputs":[],"source":["# The tokenizer converts a sentence to a sequence of tokens. Here's an example:\n","text = \"Here is an example sentence that I want to tokenize.\"\n","tokenized_text = tokenizer.tokenize(text)\n","print(tokenized_text)"]},{"cell_type":"markdown","metadata":{"id":"AihvrFWcSzd6"},"source":["Now we tokenize every review in the IMDB dataset. This may take a minute to finish."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IeM20UdZ59i1"},"outputs":[],"source":["df['tokens'] = df[DATA_COLUMN].apply(lambda x : tokenizer.tokenize(x))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X798BKV_Co71"},"outputs":[],"source":["# An example of how the tokens for a review look like:\n","print(df['tokens'][2])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JhRfWt7S7lsE"},"outputs":[],"source":["# Some reviews are long. For example:\n","len(df['tokens'][0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KQlSpWu_n4RY","tags":[]},"outputs":[],"source":["# We now truncate any review with >=(MAX_SEQ_LENGTH-2) tokens.\n","# And add special tokens [CLS] and [SEP].\n","\n","def truncate_and_add(x, max_seq_length):\n","  a = [\"[CLS]\"] + x\n","  if len(a)>max_seq_length-1:\n","    a[max_seq_length-1] = \"[SEP]\"\n","    return a[:max_seq_length]\n","  else:\n","    return a + [\"[SEP]\"]\n","\n","df['tokens'] = df['tokens'].apply(lambda x : truncate_and_add(x, max_seq_length))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bmQkpN6llqlM"},"outputs":[],"source":["len(df['tokens'][0])"]},{"cell_type":"markdown","metadata":{"id":"ul1Y5afr5GCJ"},"source":["## Converting tokens to BERT input format"]},{"cell_type":"markdown","metadata":{"id":"y9PNZoFj3N6q"},"source":["We'll need to transform our data into a format BERT understands. This involves two steps. First, we create  `InputExamples` using the constructor provided in the BERT library.\n","\n","- `text_a` is the text we want to classify, which in this case, is the `review` field in our Dataframe. \n","- `text_b` is used if we're training a model to understand the relationship between sentences (i.e. is `text_b` a translation of `text_a`? Is `text_b` an answer to the question asked by `text_a`?). This doesn't apply to our task, so we can leave `text_b` blank.\n","- `label` is the target in supervised learning, which is `sentiment` in our example"]},{"cell_type":"markdown","metadata":{"id":"tU2OpvYrRFNf"},"source":["To use BERT embedding, we need to convert the tokens of each text input into the following format:\n"," - input token ids (tokenizer converts tokens using vocab file)\n"," - input masks (1 for useful tokens, 0 for padding)\n"," - segment ids (for 2 text training: 0 for the first one, 1 for the second one)\n"]},{"cell_type":"markdown","metadata":{"id":"BFDpzy1-STOh"},"source":["Define some functions for ease of preprocessing:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e4Y_r3lmFO1E"},"outputs":[],"source":["def get_ids(tokens, tokenizer, max_seq_length):\n","    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n","    token_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n","    return np.array(token_ids, dtype=np.int32)\n","    \n","def get_masks(tokens, max_seq_length):\n","    token_masks = [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n","    return np.array(token_masks, dtype=np.int32)\n","\n","def get_segments(tokens, max_seq_length):\n","    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n","    segments = []\n","    current_segment_id = 0\n","    for token in tokens:\n","        segments.append(current_segment_id)\n","        if token == \"[SEP]\":\n","            current_segment_id = 1\n","    segments = segments + [0] * (max_seq_length - len(tokens))\n","    return np.array(segments, dtype=np.int32)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b_vqng48LuTs"},"outputs":[],"source":["df['ids'] = df['tokens'].apply(lambda x : get_ids(x, tokenizer, max_seq_length))\n","df['masks'] = df['tokens'].apply(lambda x : get_masks(x, max_seq_length))\n","df['segments'] = df['tokens'].apply(lambda x : get_segments(x, max_seq_length))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sb4GSTEkMgE3"},"outputs":[],"source":["# Let's see what the first movie review is now converted to:\n","df.iloc[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nnaj5QdqleTS"},"outputs":[],"source":["# Now assemble the data as required by the definition of BERT inputs\n","n = df.shape[0]\n","all_ids = np.zeros(shape=(n,max_seq_length))\n","all_masks = np.zeros(shape=(n,max_seq_length)) \n","all_segments = np.zeros(shape=(n,max_seq_length))\n","i = 0\n","for index, row in df.iterrows():\n","  all_ids[i] = row.ids\n","  all_masks[i] = row.masks\n","  all_segments[i] = row.segments\n","  i += 1"]},{"cell_type":"markdown","metadata":{"id":"mi2mj4EUTi0X"},"source":["\n","## Using the pre-trained BERT model for sentence embedding"]},{"cell_type":"markdown","metadata":{"id":"u-RTA4QosF1H"},"source":["BERT converts each text input (in our example, a tokenized movie review) into the following.\n","* **pooled output** (also called pooled embedding, sentence embedding): this is a vector of size `768`, which represents the whole sentence.\n","* **sequence outputs** (also called sequence embeddings, word embeddings): this is a matrix of size `[max_seq_length, 768]`, where each token is now represented by a vector of size `768`.\n","\n","**For sentiment analysis, we only need the pooled output.**\n","\n","Similar to other deep learning models, BERT doesn't transform text one record at a time. Instead, BERT takes a batch of texts (e.g., a batch of movie reviews in our case) and convert them all at once. Thus the output shapes are:\n"," - pooled output of shape `[batch_size, 768]` with representations for the entire input sequences\n"," - sequence output of shape `[batch_size, max_seq_length, 768]`"]},{"cell_type":"markdown","metadata":{"id":"rH0bhH2S9cRs"},"source":["### A big data problem\n","\n","The output size from BERT can be huge. For example, in our dataset of 10000 movie reviews, where each review has a (truncated) length of 256, the total size of sequence embeddings is: `10000 * 256 * 768 * 4 ~= 8 Gigabyte`. This is too large to fit in the memory of most personal computers. So the following single-line code will likely trigger a \"ResourceExhaustedError\".\n","\n","`pool_embs, seq_embs = model.predict([all_ids,all_masks,all_segments])`\n","\n","Below is an workaround to avoid this bid data problem. We process our data 1000 records a time, i.e., set batch size at 1000. After each batch is processed, discard the sequence embeddings because we don't need them, and only save the pooled embeddings."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aag7a3JDd2kP"},"outputs":[],"source":["pool_embs = np.zeros(shape=(n,768))\n","for i in np.arange(10):\n","  j = i*1000\n","  pool_embs[j:j+1000], seq_embs = model.predict([all_ids[j:j+1000],\n","                                                 all_masks[j:j+1000],\n","                                                 all_segments[j:j+1000]])\n","  print(f'{i+1}/10 of the data processed.')"]},{"cell_type":"markdown","metadata":{"id":"8d5h6Dz0lqlO"},"source":["***The previous code cell took 34 minutes to run on JupyterLab my PC. Using Google Colab, it took 8 minutes.***"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DP43272orwSR"},"outputs":[],"source":["pool_embs.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ktdixe_2tfyx","tags":[]},"outputs":[],"source":["pool_embs[0]"]},{"cell_type":"markdown","metadata":{"id":"r5HcG7LpAAZE"},"source":["## Assembling a new dataset with features extracted by BERT\n","\n","For each text, the corresponding pooled output is a vector of 768 numbers that summaries this whole text. We can now treat these 768 numbers as features extracted by BERT. Let's assemble a new DataFrame with these figures and the sentiment data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jxzOuyZd_O92"},"outputs":[],"source":["feature_df = pd.DataFrame(pool_embs)\n","feature_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pZi0XUqcrRay","tags":[]},"outputs":[],"source":["feature_df['sentiment'] = df['sentiment']\n","feature_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"46LGa84M8pOw"},"outputs":[],"source":["# Warning: this file will be large, about 150MB\n","feature_df.to_csv(\"/content/drive/MyDrive/AMA/12_NLP_using_BERT/IMDB_small_BERT.csv\", index=False)"]},{"cell_type":"markdown","metadata":{"id":"bgAk3J6zDf2y"},"source":["## Building and evaluating the prediction model\n","\n","The rest is similar to what we did with the business loan dataset earlier this semester. I'll use the simple logistic regression model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WSd-GQC1VolF"},"outputs":[],"source":["from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gRVdYop0VKjN"},"outputs":[],"source":["X = feature_df.drop(columns=['sentiment'])\n","y = feature_df['sentiment']\n","\n","# reserve 30% dataset as testing\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, \n","                                                    random_state=1,\n","                                                    stratify=y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IczZQOIUCqWD"},"outputs":[],"source":["model2 = models.Sequential()\n","model2.add(layers.Dense(128, activation='relu', input_dim=768))\n","# model2.add(layers.Dropout(0.5))\n","model2.add(layers.Dense(1, activation='sigmoid'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h9KN2l8HWXJa"},"outputs":[],"source":["model2.compile(optimizer='adam',\n","              loss='binary_crossentropy',\n","              metrics=['acc'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NlwtEMbuXg3k"},"outputs":[],"source":["model2.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MJZGmll0Wu0b"},"outputs":[],"source":["model2.fit(X_train, y_train, epochs=30)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_RRh4XkYXuw7"},"outputs":[],"source":["test_loss, test_acc = model2.evaluate(X_test,  y_test, verbose=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W-sBWYvAXzer"},"outputs":[],"source":["# prediction\n","model2.predict(X_test.iloc[[0]])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZdCN6PUCYnGm"},"outputs":[],"source":["print(y_test[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6C3n7WJRlqlQ"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"NLP_using_TF2_and_BERT.ipynb","provenance":[{"file_id":"1hMLd5-r82FrnFnBub-B-fVW78Px4KPX1","timestamp":1587333874208}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":0}