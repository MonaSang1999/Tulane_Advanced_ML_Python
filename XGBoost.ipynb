{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B-Zz_NbBpmdG"
   },
   "source": [
    "# (State-of-the-Art) Learning Algorithms Based on Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uzMEYkOCena1"
   },
   "source": [
    "In recent years and for numerical datasets (i.e. not multimedia based), supervised learning algorithms based on **gradient boosting**, and ***with careful hyperparameter training***, frequently won open data-classification contests organized by kaggle.com and other forums when classification accuracy or ROC AUC is the metric. These algorithms are the state-of-the art learning algorithms for traditional supervised learning.\n",
    "\n",
    "Gradient boosting, as we will study below, is a boosting technique that can be applied to any weak learning algorithms (why weak?). That said, in practice it is typically applied to decision trees -- the result of which is called **Gradient Boosted Decision Trees (GBDT)**.\n",
    "\n",
    "In this lecture, we will try the following popular GBDT implementations that winners at open contests frequently use:\n",
    "+ **XGBoost**, the go-to choice in the last decade, until it was taken over by the next one\n",
    "+ **LightGBM**, similar in performance to XGBoost but much faster for large data\n",
    "+ **HistGradientBoostingClassifier**, scikit-learn's adaption of LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WWyVYJ04yo0q",
    "tags": []
   },
   "source": [
    "## XGBoost\n",
    "\n",
    "XGBoost standards for 'Extreme Gradient Boosting':\n",
    "+ Extreme: it mainly means ***fast*** through a variety of computational and resource-management tricks (not required)\n",
    "\n",
    "(Not required, but fun to watch at your spare time) XGBoost uses a special type of decision tree called XGBoost tree -- see (https://www.youtube.com/watch?v=OtD8wVaFm6E&t=11s) for an excellent tutorial on this tree structure.\n",
    "\n",
    "See (https://xgboost.readthedocs.io/en/latest/) for documentation of XGBoost. We will discuss a few of the key parameters of XGBoost when we try the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 2284,
     "status": "ok",
     "timestamp": 1619472764834,
     "user": {
      "displayName": "Xianjun Geng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjp2zNPbIYU0DaDSSzFs4-oEwNRuLg3LhJILAgh3RY=s64",
      "userId": "03603659341670711497"
     },
     "user_tz": 300
    },
    "id": "b8AiVuH_phgT"
   },
   "outputs": [],
   "source": [
    "# load necessary Python packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('max_columns', 50)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 687,
     "status": "ok",
     "timestamp": 1619472768528,
     "user": {
      "displayName": "Xianjun Geng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjp2zNPbIYU0DaDSSzFs4-oEwNRuLg3LhJILAgh3RY=s64",
      "userId": "03603659341670711497"
     },
     "user_tz": 300
    },
    "id": "jsAPeTuHphgT"
   },
   "outputs": [],
   "source": [
    "# Load the balanced LendingClub dataset\n",
    "\n",
    "df = pd.read_csv('LendingClub_balanced.csv')\n",
    "\n",
    "X = df.drop(columns=['not_fully_paid'])\n",
    "y = df['not_fully_paid']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=365)\n",
    "X_train = X_train.copy()\n",
    "X_test = X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "g8IeiOJS1r7y"
   },
   "outputs": [],
   "source": [
    "# The xgboost package is separate from sklearn. That said, it provides\n",
    "# xgb.XGBClassifier as a sklearn-compatible interface.\n",
    "\n",
    "# Note: you might need to first install the xgboost package in anaconda\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "xgb_sklearn = xgb.XGBClassifier(verbosity=0, use_label_encoder=False, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "VtxidJMFKQJw",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters are: {'max_depth': 2, 'n_estimators': 50}\n",
      "The cross-validation accuracy is: 0.6146\n",
      "The testing accuracy is: 0.6156\n",
      "The confusion matrix is:\n",
      "[[189 109]\n",
      " [127 189]]\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning of xgboost\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': np.arange(50,250,50),\n",
    "    'max_depth': np.arange(2,7,2),\n",
    "}    \n",
    "\n",
    "grid = GridSearchCV(estimator = xgb_sklearn, param_grid = param_grid, cv=3)\n",
    "grid.fit(X_train, y_train)\n",
    "print(f\"Best parameters are: {grid.best_params_}\")\n",
    "print(f\"The cross-validation accuracy is: {grid.best_score_:.4}\")\n",
    "\n",
    "# evaluation\n",
    "y_predict = grid.best_estimator_.predict(X_test)\n",
    "print(f\"The testing accuracy is: {accuracy_score(y_test, y_predict):.4}\")\n",
    "print(\"The confusion matrix is:\")\n",
    "cm = confusion_matrix(y_test, y_predict)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "613YDNzcyo0s"
   },
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JERIHj3Fyo0s"
   },
   "source": [
    "[Researchers in Microsoft](https://www.microsoft.com/en-us/research/project/lightgbm/) created LightGBM in 2016. It quickly gained popularity in Kaggle.com competitions as it produces comparable performance with XGBoost, yet often faster -- sometimes several magnitude of speed difference. (And this is remarkable given that XGBoost was already considered fast!)\n",
    "\n",
    "The key reason behind this huge speed improvement is LightGBM's use of histogram-based algorithms: *for each continuous feature, LightGBM buckets it into discrete bins*. This greatly speeds up the tree growing process because there are far less number of branch separation points to consider. LightGBM also use a lot [other tricks](https://lightgbm.readthedocs.io/en/latest/Features.html#) to speed up the training process.\n",
    "\n",
    "See (https://lightgbm.readthedocs.io/en/latest/) for documentation of LightGBM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mt01tuyl1-SS"
   },
   "source": [
    "Note: Recently scikit-klearn introduced a new [experimental learning algorithm](https://scikit-learn.org/stable/modules/ensemble.html#histogram-based-gradient-boosting) called `HistGradientBoostingClassifier` that is inspired by LightGBM. However, I don't yet see much adoption of it in practice. `HistGradientBoostingClassifier` also appears to perform worse than XGBoost and LightGBM when I tried.\n",
    "\n",
    "Let's try LightGBM below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "jilk2ubgyo0t"
   },
   "outputs": [],
   "source": [
    "# You might need to first install lightgbm package in anaconda\n",
    "\n",
    "import lightgbm as lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "fR0-lrM0yo0u",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters are: {'max_depth': 2, 'n_estimators': 200}\n",
      "The cross-validation accuracy is: 0.6183\n",
      "The testing accuracy is: 0.6091\n",
      "The confusion matrix is:\n",
      "[[186 112]\n",
      " [128 188]]\n"
     ]
    }
   ],
   "source": [
    "lgbm_sklearn = lgbm.LGBMClassifier(random_state=1)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': np.arange(50,250,50),\n",
    "    'max_depth': np.arange(2,7,2),\n",
    "}    \n",
    "\n",
    "grid = GridSearchCV(estimator = lgbm_sklearn, param_grid = param_grid, cv=3)\n",
    "grid.fit(X_train, y_train)\n",
    "print(f\"Best parameters are: {grid.best_params_}\")\n",
    "print(f\"The cross-validation accuracy is: {grid.best_score_:.4}\")\n",
    "\n",
    "# evaluation\n",
    "y_predict = grid.best_estimator_.predict(X_test)\n",
    "print(f\"The testing accuracy is: {accuracy_score(y_test, y_predict):.4}\")\n",
    "print(\"The confusion matrix is:\")\n",
    "cm = confusion_matrix(y_test, y_predict)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wfdk7DOvJRzV"
   },
   "source": [
    "### Hyperparameter tuning of LightGBM using ROC AUC as performance metric\n",
    "\n",
    "If we don't explicitly specify the performance metric for classification, `GridSearchCV()` used 'accuracy' as the default metric. Now let's try an alternative metric of roc_auc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "VtxidJMFKQJw",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "VtxidJMFKQJw",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters are: {'max_depth': 2, 'n_estimators': 50}\n",
      "The cross-validation ROC AUC is: 0.6705\n",
      "The testing ROC AUC is: 0.6492\n",
      "The confusion matrix is:\n",
      "[[186 112]\n",
      " [128 188]]\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': np.arange(50,250,50),\n",
    "    'max_depth': np.arange(2,7,2),\n",
    "}    \n",
    "\n",
    "grid = GridSearchCV(estimator = lgbm_sklearn, param_grid = param_grid, scoring='roc_auc', cv=3)\n",
    "grid.fit(X_train, y_train)\n",
    "print(f\"Best parameters are: {grid.best_params_}\")\n",
    "print(f\"The cross-validation ROC AUC is: {grid.best_score_:.4}\")\n",
    "\n",
    "# evaluation\n",
    "y_predict = grid.best_estimator_.predict(X_test)\n",
    "y_predict_proba = grid.best_estimator_.predict_proba(X_test)[:,1]\n",
    "print(f\"The testing ROC AUC is: {roc_auc_score(y_test, y_predict_proba):.4}\")\n",
    "print(\"The confusion matrix is:\")\n",
    "cm = confusion_matrix(y_test, y_predict)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UG0ePsr1IKwN"
   },
   "source": [
    "## Summary\n",
    "\n",
    "Hyperparameter tuning helps relieve data scientists from the repetitive work of fine-tuning a model by experimenting with various hyperparameter values. This is widely adopted in today's analytical practice."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMYquq/jiL6dxDHhf0Kq9WV",
   "collapsed_sections": [],
   "name": "ML_5_HyperparameterTuning_and_XGBoost_filled.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
